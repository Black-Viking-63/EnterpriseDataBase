# EnterpriseDataBase

Данный репозиторий содержит решение второй лабораторной работы по курсу "Корпоративные базы данных".


# Описание лабораторной работы

В данной лабораторной работе необходимо было реализовать задачу машинного обучения для базы данных.<br>В качестве задачи машинного обучения в лабораторной работе решалась задача классификации данных.

# Задание на лабораторную работу

1. Сформулировать задачу машинного обучения для БД из 1 лабораторной, которую собираетесь решать.
2. Разделить данные из БД на тренировочные и тестовые.
3. Определить вектор признаков для данных вашей предметной области. Самая сложная часть работы, от которой зависит дальнейшая эффективность машинного обучения, советую ориентироваться на существующие принципы выбора признаков и предварительной обработки данных в выбранной предметной области.
4. Выбрать тип классификатора: от самых простых kNN и линейного до нейронных сетей и RandomForest.
5. Оценить оптимальные значения гиперпараметров. Построить соответствующие зависимости качества распознавания от гиперпараметров.
6. Обучить классификатор на тренировочной выборке и оценить его эффективность на тестовой выборке.

# Выполнение лабораторной работы
Выполнение лабораторной работы было раздлено на 5 основных этапов
* Этап 1 - Подготовительный этап, на котором производим подключение библиотек, Google Drive, а также загрузку данных датасета из файла.
    В качестве источника данных был использован dataset данных, с сайта  `https://www.kaggle.com/` по [ссылке](https://www.kaggle.com/datasets/pritsheta/diabetes-dataset).
* Этап 2 - Производится подготовка данных: выделение вектора признаков, разделение данных на тестовую и тренировочную выборки, а так же нормализация полученных наборов.
* Этап 3 - Производится выбор классификатора, при помощи которого и будем производить классификацию данных. В качестве классификаторов я использовал следующие:
    + kNN - (k Nearest Neighbor или k Ближайших Соседей) - простейший алгоритм классификации, используемый в машинном обучения;
    + DecisionTreeClassifier - алгоритм классификации, построенный на основе дерева решений;
    + RandomForestClassifier - алгоритм классификации, построенный на основе ансамбля деревьев решений.
* Этап 4 - Производится подбор оптимальных значений гиперпараметров, для выполнения данноый операции использовалась функция GridSearchCV.
* Этап 5 - Производится выбор классификатора с учетом гиперпараметров.

## Этапы 3-5
Рассмотрим более подробно этапы 3-5, поскольку оснонвая часть работы выполнена на данных этапах.
### Этап 3
На данном этапе мы производим инициализацию и обучение классификатора с параметрами по умолчанию, после чего производим запуск классификатора. Например kNN: 
```python
classifierKNN = KNeighborsClassifier()
classifierKNN.fit(X_train_scaler, y_train)
classifierPredictionKNN = classifierKNN.predict(X_test_scaler)
```
Важным шагом на данном этапе является проверка точности работы классификатора, собственно по данному параметру мы и будем оценивать работу классификатора.
```python
accuracy_score(y_test, classifierPredictionKNN)*100
```
### Этап 4
На данном этапе мы производим подбор оптимальных гиперпараметров при помощи GridSearchCV, а так же инициализируем классификатор с оптимальными гиперпараметрами.
```python
DecisionTreeParams = {
    "max_depth": np.linspace(1, 32, 32, endpoint=True),
    "min_samples_split": np.linspace(0.01, 0.1, 10, endpoint=True),
    "min_samples_leaf": np.linspace(0.01, 0.1, 10, endpoint=True),
}
DecisionTreeGSCV = GridSearchCV(classifierDTC, DecisionTreeParams)
```
### Этап 5
На финальном этапе работы с классификаторами, запустим их с учетом подобранных оптимальных гиперпараметров, а так же посмотрим точность предсказания. Например:
```python
RandomForestGSCV.fit(X_train_scaler, y_train)
print(RandomForestGSCV.best_estimator_)
RandomForestGSCV_Predict = RandomForestGSCV.predict(X_test_scaler)
accuracy_score(y_test, RandomForestGSCV_Predict) * 100
```
# Результаты
Рассмотрим полученные результаты в ходе выполнения лабораторной работы. Для удобства изучения, они были сведены в таблицу.
| --- | kNN | Decision<br>Tree<br>Classifier | Random<br>Forest<br>Classifier |
|:---:|:---:|:---:|:---:|
| <b>Without<br>hyperparameter<br>tuning<b> | 71.75% | 73.23% | 77.69% |
| <b>With<br>hyperparameter<br>tuning<b> | 75.09% | 72.86% | 76.95% |


Нетрудно заметить, что полученные результаты совсем неоднозначны. Самым точным классифиатором является RandomForestClassifier, несмотря на потерю 0.74% точности, произошедшую в процессе настройки гиперпарамтров. Наибольший прирост точности показал классификатор kNN, который повысил точность на 3.34%, после настройки гипер параметров
   

# Итоги
Подводя итоги можно абсолютно точно отдать первое место RandomForestClassifier, второе место занимает kNN, на третьем месте DecisionTreeClassifier. Данная оценка носит чисто субъективный характер и зависит только от мнения автора. Так же хочется сказать не много про полученные результаты. Точность предсказаний классифиактора зависит от многих факторов, таких как, процентное соотношение данных при разбиении выборки данных на тестовый и тренировочный пакеты, нормализация данных, подбор гиперпараметров, причем поодбор гиперпараметров не всегде может гарантировать увеличение тосности предсказаний классификатора, чтомы смогли и увидеть на примере DecisionTreeClassifier.
